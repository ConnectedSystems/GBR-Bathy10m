"""
Cluster identified suitable locations
"""

using Rasters
import GeoDataFrames as GDF
import ArchGDAL as AG
using CSV

using DataFrames, Statistics, StatsBase
using Clustering, Distances
using BlackBoxOptim, Random

using Clustering

include("common.jl")

"""Identify column IDs which hold Geometry data types."""
function get_geometry_col(df::DataFrame)
    col_types = string.(eltype.(eachcol(df)))
    return first(findall(contains.(col_types, "Geometry")))
end

"""Get the geometry data as a vector"""
function get_geometry(df::DataFrame)::Vector
    geom_col = get_geometry_col(df)
    if sum(geom_col) == 0
        error("No geometry data found")
    end

    return df[:, geom_col]
end


"""
    centroids(df::DataFrame)

Extract and return long/lat from a GeoDataFrame.

# Arguments
- `df` : GeoDataFrame

# Returns
Array of tuples (x, y), where x and y relate to long and lat respectively.
"""
function centroids(df::DataFrame)::Vector{Tuple{Float64,Float64}}
    site_centroids::Vector = AG.centroid.(get_geometry(df))
    return collect(zip(AG.getx.(site_centroids, 0), AG.gety.(site_centroids, 0)))
end


"""
    create_distance_matrix(site_data::DataFrame)::Matrix

Calculate matrix of unique distances between locations.

# Returns
Distance between locations in meters
"""
function create_distance_matrix(site_data::DataFrame)::Matrix{Float64}
    site_centroids = centroids(site_data)
    longitudes = first.(site_centroids)
    latitudes = last.(site_centroids)

    n_sites = size(site_data, 1)
    dist = zeros(n_sites, n_sites)
    for ii in axes(dist, 1)
        for jj in axes(dist, 2)
            if ii == jj
                continue
            end

            @views dist[ii, jj] = haversine(
                (longitudes[ii], latitudes[ii]), (longitudes[jj], latitudes[jj])
            )
        end
    end

    return dist
end

"""Generate DxN point data based on location centroids."""
function create_location_points(gdf::DataFrame)
    all_centroids = centroids(gdf)
    all_locs = Matrix([first.(all_centroids) last.(all_centroids)]')

    return all_locs
end

"""
    add_score(df::DataFrame)::DataFrame

Add a score column to the DataFrame based on flat and slope scores.

# Arguments
- `df`: DataFrame

# Returns
DataFrame with added score column.
"""
function add_score(df::DataFrame)::DataFrame
    score = df[:, :flat_scr] .+ df[:, :slope_scr]
    df.score = score
    return df
end

"""
    add_suitable_area(df::DataFrame)::DataFrame

Add a suitable_area column to the DataFrame based on flat and slope areas.

# Arguments
- `df`: DataFrame

# Returns
DataFrame with added suitable_area column.
"""
function add_suitable_area(df::DataFrame)::DataFrame
    suitable_area = df[:,:flat_ha] .+ df[:,:slope_ha]
    df.suitable_area = suitable_area
    return df
end

# function add_combined_col(df::DataFrame, colnames_to_combine::Vector{String}, colname_resultant::String)::DataFrame
#     # Initialize a combined column with zeros
#     combined_col = zeros(Float64, nrow(df))

#     # Iterate through each column name in the colnames vector and add the values to the combined column
#     for col in colnames_to_combine
#         combined_col .+= df[:, col]
#     end

#     # Add the combined column as a new column with the name specified by colname_resultant
#     df[:, Symbol(colname_resultant)] = combined_col

#     return df
# end

# function add_cols(df::DataFrame)::DataFrame
#     score_cols = ["flat_scr", "slope_scr"]
#     df = add_combined_col(df, score_cols, "score")

#     suitable_area_cols = ["flat_ha", "slope_ha"]
#     df = add_combined_col(df, suitable_area_cols, "suitable_area")
#     return df
# end

"""
    sort_limit_areas(targets::DataFrame; serviceable_area::Int64=1000)::DataFrame

Sort targets by score, removing those with score <= 0
limit the total/cumulative serviceable area to a specified value.

# Arguments
- `targets::DataFrame`: Targets with columns suitable_area and score
- `serviceable_area::Int64=1000`: Maximum serviceable area

# Returns
- `selected_rows::DataFrame`: Selected rows with cumulative area <= serviceable_area
"""
# function sort_limit_areas(targets::DataFrame; serviceable_area::Int64=1000)::DataFrame
#     threshold_area = 0
#     selected_rows = DataFrame()

#     targets = add_score(targets)
#     targets = add_suitable_area(targets)

#     # Sort targets by score, removing those with score <= 0
#     sorted_targets = sort(targets[targets.score .> 0, :], :score, rev=true)

#     # Iterate through sorted targets to accumulate areas until reaching a threshold
#     for row in eachrow(sorted_targets)
#         threshold_area += row.suitable_area
#         if threshold_area <= serviceable_area
#             push!(selected_rows, row)
#         else
#             # println("Serviceable area threshold reached at $(threshold_area) ha.")
#             break
#         end

#     end
#     println("Serviceable area threshold reached at $(threshold_area) ha.")
#     return selected_rows
# end

function opt_kmeans(X, locations, dists)
    n_clusters = X[1]

        local clusters
    try
        clusters = kmeans(locations, floor(Int64, n_clusters), display=:none)
        if !clusters.converged
            # Return worst score if k-means has not converged
            return 1.0
        end
    catch err
        if err isa BoundsError
            return 1.0
        else
            rethrow(err)
        end
    end

    assignments = clusters.assignments

    sil_score = -1.0
    try
        sil_score = silhouettes(assignments, dists)
    catch err
        if !(err isa ArgumentError)
            rethrow(err)
        else
        # All locations assigned to a single cluster so assign worst score
        sil_score = -1.0
        end
    end

    # Optimization direction is toward the minimum, so invert score.
    return -median(sil_score)
end

"""
    cluster(gpkg_path::String, output_path::String)

Cluster locations indicated in the geopackage file.
Limit areas serviced by score.

# Arguments
- `gpkg_path` : path to geopackage
- `output_path` : desired path to output geopackage
- `n_steps` : maximum number of optimization steps
- `epsilon` : convergence tolerance
- `serviceable_area` : maximum serviceable area"""
function cluster(gpkg_path::String, output_path::String; n_steps=100, epsilon=0.4, min_clusters=1, serviceable_area=typemax(Int64))
    Random.seed!(101)

    gdf = GDF.read(gpkg_path)

    # gdf = sort_limit_areas(gdf; serviceable_area=serviceable_area)
    ## remove any area = units! ^^

    dist_mat = create_distance_matrix(gdf)
    all_locs = create_location_points(gdf)

    n_locs = length(unique(gdf.UNIQUE_ID))
    dist_bnds = [(min_clusters,n_locs)
        # n clusters to find (n_locs + 1 to 3*n_reefs)
        # Upper bound has +1 as we take the floor of the sampled value.
        # (n_locs+1, (n_locs*3)+1),
    ]

    opt_func = x -> opt_kmeans(x, all_locs, dist_mat)

    res = bboptimize(
        opt_func;
        SearchRange=dist_bnds,
        MaxSteps=n_steps,
        Ïµ=epsilon,
    )
    best_params = best_candidate(res)

    clusters = kmeans(all_locs, floor(Int64, best_params[1]), display=:none)
    assignments = clusters.assignments
    sil_score = silhouettes(assignments, dist_mat)
    @info "Produced $(maximum(assignments)) clusters with median Silhouette score of: $(median(sil_score))"
    @info "Silhouette score ranges from -1 to 1, with higher values being more desirable."
    @info "Values above 0.7 are regarded as strong, values over 0.5 are reasonable, and"
    @info "values below 0.25 are weak."
    #Qualitative indicators taken from https://en.wikipedia.org/wiki/Silhouette_(clustering)

    gdf.cluster_id = assignments
    GDF.write(output_path, gdf; geom_columns=(:geometry, ))
end

n_steps=100
epsilon=0.4
min_clusters=1
serviceable_area=typemax(Int64)

cluster(joinpath(MPA_QGIS_DIR, "reef_suitability.gpkg"), joinpath(MPA_OUTPUT_DIR, "clustered_reef_suitability_$(serviceable_area)Ha_min$(min_clusters)clust_$(n_steps)_$(epsilon).gpkg"); n_steps=n_steps, epsilon=epsilon, min_clusters=min_clusters, serviceable_area = serviceable_area)
